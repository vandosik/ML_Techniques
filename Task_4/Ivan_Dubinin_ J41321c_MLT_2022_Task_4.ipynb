{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "pmGD_DRv7LZa",
    "outputId": "beea3e59-b48c-4b3d-f03b-71e2b11f4763"
   },
   "source": [
    "# Task\n",
    "\n",
    "1. Download Alice in Wonderland by Lewis Carroll from Project Gutenberg's website http://www.gutenberg.org/files/11/11-0.txt\n",
    "2. Perform any necessary preprocessing on the text, including converting to lower case, removing stop words, numbers / non-alphabetic characters, lemmatization.\n",
    "3. Find Top 10 most important (for example, in terms of TF-IDF metric) words from each chapter in the text (not \"Alice\"); how would you name each chapter according to the identified tokens?\n",
    "4. Find the Top 10 most used verbs in sentences with Alice. What does Alice do most often?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alice example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gEdTiNbO7gzt"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vando\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vando\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vando\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\vando\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import wordnet \n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "nltk.download(\"wordnet\")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download Alice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "AIranMYWFKnJ",
    "outputId": "0ba39dfd-6a5b-4281-aea5-81de44ef0cfd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffThe Project Gutenberg eBook of Alice’s Adventures in Wonderland, by Lewis Carroll\\n\\nThis eBook is for the use of anyone anywhere in the United States and\\nmost other parts of the world at no cost and with almost no restrictions\\nwhatsoever. You may copy it, give it away or re-use it under the terms\\nof the Project Gutenberg License included with this eBook or online at\\nwww.gutenberg.org. If you are not located in the United States, you\\nwill have to check the laws of the country where you are located before\\nusing this eBook.\\n\\nTitle: Alice’s Adventures in Wonderland\\n\\nAuthor: Lewis Carroll\\n\\nRelease Date: January, 1991 [eBook #11]\\n[Most recently updated: October 12, 2020]\\n\\nLanguage: English\\n\\nCharacter set encoding: UTF-8\\n\\nProduced by: Arthur DiBianca and David Widger\\n\\n*** START OF THE PROJECT GUTENBERG EBOOK ALICE’S ADVENTURES IN WONDERLAND ***\\n\\n[Illustration]\\n\\n\\n\\n\\nAlice’s Adventures in Wonderland\\n\\nby Lewis Carroll\\n\\nTHE MILLENNIUM FULCRUM EDITION 3.0\\n\\nContents\\n\\n CHAPTER I.     Down the Rabbit-'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'alice_text.txt'\n",
    "with open(filename, encoding='utf-8') as f:\n",
    "    alice_text = f.read()\n",
    "alice_text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Perform any necessary preprocessing on the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDown the Rabbit-Hole\\n\\n\\nAlice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to do: once or twice she had peeped into\\nthe book her sister was reading, but it had no pictures or\\nconversations in it, “and what is the use of a book,” thought Alice\\n“without pictures or conversations?”\\n\\nSo she was considering in her own mind (as well as she could, for the\\nhot day made her feel very sleepy and stupid), whether the pleasure of\\nmaking a daisy-chain would be w'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove irrelevant text\n",
    "alice_text_main = alice_text.split('CHAPTER I.')[2]\n",
    "alice_text_main = alice_text_main.split('THE END')[0]\n",
    "alice_text_main[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "awkpjqGSEfYb",
    "outputId": "3e94c688-da31-41e8-9705-fdc41a8eb6ec"
   },
   "outputs": [],
   "source": [
    "# CODE 1: \n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "# The above func was borrowed here: https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "\n",
    "def preprocess_text(base_text):\n",
    "    # Move to lower case\n",
    "    new_text = base_text.lower()\n",
    "\n",
    "    # Remove contractions\n",
    "    new_text = decontracted(new_text)\n",
    "\n",
    "    # Remove bad symbols\n",
    "    new_text = re.sub(r\"[^\\w\\s]\", \"\", new_text) # This call saves _\n",
    "    new_text = re.sub(\"_\", \" \", new_text)\n",
    "\n",
    "    # Split to tokens\n",
    "    text_tokens = new_text.split()\n",
    "    # tokens = TreebankWordTokenizer().tokenize(text) \n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    sw_tokens = [token for token in text_tokens if not token in stop_words]\n",
    "\n",
    "    #Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # Lemmatize nouns\n",
    "    lemm_tokens = [lemmatizer.lemmatize(token) for token in sw_tokens]\n",
    "    # Lemmatize verbs\n",
    "    lemm_tokens = [lemmatizer.lemmatize(token, \"v\") for token in sw_tokens]\n",
    "\n",
    "    # Collect text from tokens\n",
    "    new_text = \" \".join(lemm_tokens) # For what reason this line stands (соеденить текст обратно?)\n",
    "    \n",
    "    return new_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Find Top 10 most important words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1:\t['think' 'eat' 'say' 'little' 'bat' 'fall' 'key' 'try' 'wonder' 'happen']\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "2:\t['mouse' 'say' 'ill' 'pool' 'little' 'swim' 'think' 'cat' 'dear' 'fan']\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "3:\t['say' 'mouse' 'dodo' 'prize' 'bird' 'lory' 'ill' 'dry' 'know' 'thimble']\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "4:\t['grow' 'little' 'window' 'rabbit' 'say' 'run' 'fan' 'puppy' 'hear' 'come']\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "5:\t['say' 'caterpillar' 'pigeon' 'serpent' 'egg' 'youth' 'bite' 'try' 'think'\n",
      " 'size']\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "6:\t['say' 'cat' 'footman' 'baby' 'mad' 'duchess' 'sneeze' 'pig' 'grin'\n",
      " 'think']\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "7:\t['say' 'hatter' 'dormouse' 'march' 'hare' 'twinkle' 'draw' 'remark' 'time'\n",
      " 'know']\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "8:\t['queen' 'say' 'soldier' 'gardeners' 'look' 'king' 'hedgehog' 'cat' 'head'\n",
      " 'executioner']\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "9:\t['say' 'turtle' 'mock' 'gryphon' 'duchess' 'moral' 'queen' 'think' 'sigh'\n",
      " 'remark']\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "10:\t['turtle' 'mock' 'gryphon' 'say' 'dance' 'beautiful' 'soup' 'join'\n",
      " 'repeat' 'lobster']\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "11:\t['king' 'hatter' 'say' 'court' 'dormouse' 'witness' 'begin' 'queen'\n",
      " 'officer' 'look']\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "12:\t['say' 'king' 'jury' 'write' 'dream' 'queen' 'sister' 'mean' 'rabbit'\n",
      " 'jurymen']\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Split text on chapters\n",
    "chapters = alice_text_main.split('CHAPTER ')\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf.fit_transform(chapters)\n",
    "\n",
    "chapter_num = 1\n",
    "for chapter in chapters:\n",
    "    clear_chapter = preprocess_text(base_text = chapter)\n",
    "    clear_chapter = re.sub(r'alice', '', clear_chapter)\n",
    "    response = tfidf.transform([clear_chapter])\n",
    "    feature_array = np.array(tfidf.get_feature_names())\n",
    "    tfidf_sorting = np.argsort(response.toarray()).flatten()[::-1]\n",
    "\n",
    "    top_ten_words = feature_array[tfidf_sorting][:10]\n",
    "    print(\"\\n\"+str(chapter_num)+\":\\t\"+str(top_ten_words))\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    chapter_num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Find the Top 10 most used verbs in sentences with Alice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['say' 'think' 'know' 'run' 'come' 'make' 'begin' 'happen' 'king' 'look']\n"
     ]
    }
   ],
   "source": [
    "# tokenization - sentences case\n",
    "sents = sent_tokenize(alice_text_main)\n",
    "\n",
    "alice_sents = []\n",
    "verbs = ''\n",
    "for sentence in sents:\n",
    "    clearSentence = preprocess_text(base_text = sentence)\n",
    "    if 'alice' in clearSentence:\n",
    "        alice_sents.append(clearSentence)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "for sentence in alice_sents:\n",
    "    s = nltk.pos_tag(sentence.split())\n",
    "    for w in s:\n",
    "        if 'VB' in w[1]:\n",
    "            verbs += w[0] + ' '\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "response = tfidf.transform([verbs])\n",
    "feature_array = np.array(tfidf.get_feature_names())\n",
    "tfidf_sorting = np.argsort(response.toarray()).flatten()[::-1]\n",
    "\n",
    "print(feature_array[tfidf_sorting][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm considers word \"king\" as a verb, but originaly it is used as noun :)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Задание 7 (Сибилев Д. C41101).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
